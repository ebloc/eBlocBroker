                                    *outline*
* TASKS
** TODO log ~dataTransferIn~ in the contract
- self.eBlocBroker.decode_input(input)
- testte dataTransferIn hep [0, 0, 0, 0] olarak hesaplanmis?
** TODO patch bdrop and google-drive
** TODO verify downloaded data
** TODO convert ~ipfsID~ into ~ipfsAddress~ in the ~eBlocBrokerInterface.sol~
** TODO Test updated code in 2 machines with your test cast submitting job.
** TODO IPFS inside docker should be swarm connect from outside
** TODO Do demo video do from https://asciinema.org/a/480875 , you can copy string from the video
** TODO check ? dataPricesSetBlockNum > 0 is true at checkRegisteredData() for registered data usage
** TODO check ? in smart-contract in ~internal~ calls is if memory variable updated does it updated as well on where it called from
** TODO token
– https://docs.openzeppelin.com/contracts/2.x/erc20
– https://github.com/OpenZeppelin/openzeppelin-contracts/tree/master/contracts/token/ERC20
** TODO get_transaction_receipt console'a ekle
** TODO rootless docker in the compute nodes
https://serverfault.com/a/1078434/395276
** TODO https://www.mturk.com
** TODO apply patched for all the folders
** TODO Later do, when cache time is completed:
   oc.decline_remote_share(int(<share_id>)) to cancel shared folder at
   end_code or after some time later
** TODO /* Sets the job's state (stateCode) which is obtained from Slurm */
Slurm status should be set on the receiveDeposit call not setJobStatus
** TODO DAS2-fs1-2003-1.swf
   Starting from // 5955  9447070
   __ https://www.cse.huji.ac.il/labs/parallel/workload/l_das2/index.html
** TODO oc may have "owncloud.owncloud.HTTPResponseError: HTTP error: 401" error
** TODO Some code should go into utils.py
   Some functıons are public that should carried into ~utils.py~
** TODO for workflow visualization: https://github.com/mermaid-js/mermaid
** TODO remove _balance? could be not needed.
** TODO BUGS
#+begin_src python
Failed to share file: googleapi: Error 403: Rate limit exceeded. User message: "Sorry, you have exceeded your sharing quota.", sharingRateLimitExceeded
#+end_src
** TODO fetch b2drop empty size
** TODO investigate store web3 ebb object and read reread from it using picle
** TODO
+ hide ipfs progress in submitting jobs

+ Failed to get about: Get "https://www.googleapis.com/drive/v3/about?alt=json&fields=maxImportSizes%2CmaxUploadSize%2CstorageQuota%2Cuser": oauth2: cannot fetch token: 400 Bad Request
Response: {
  "error": "invalid_grant",
  "error_description": "Token has been expired or revoked."
}
** TODO Fetch ipfs repo dir from the cfg.yaml for each instance

** TODO =warning: timeout function took too long events since bn=20990633 -- counter=0:00:54 ...   Awaiting transaction in the mempool... \=

timeout error during job submissions: start submitting jobs than stop docker instance and make it
work in that condition.
** TODO errors during test:

- Exception: b2drop download error
ReadTimeout: HTTPConnectionPool(host='berg-cmpe-boun.duckdns.org', port=8545): Read timed out. (read timeout=30)

- gdrive: Exception: Requested size to download the source-code and data files is greater than the given amount
#+begin_src python
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/alper/ebloc-broker/broker/drivers/gdrive.py:317 in run                                     │
│                                                                                                  │
│   314 │   │   try:                                                                               │
│   315 │   │   │   if os.path.isdir(self.results_folder):                                         │
│   316 │   │   │   │   # attempt to download the source code                                      │
│ ❱ 317 │   │   │   │   target = self.get_data(key=self.job_key, _id=0, is_job_key=True)           │
│   318 │   │   │                                                                                  │
│   319 │   │   │   if not os.path.isdir(f"{target}/.git"):                                        │
│   320 │   │   │   │   # log(f"warning: .git folder does not exist within {target}")              │
│                                                                                                  │
│ /home/alper/ebloc-broker/broker/drivers/gdrive.py:217 in get_data                                │
│                                                                                                  │
│   214 │   │   │   raise e                                                                        │
│   215 │   │                                                                                      │
│   216 │   │   if is_job_key:                                                                     │
│ ❱ 217 │   │   │   gdrive_info = self.pre_data_check(key)                                         │
│   218 │   │   │   name = gdrive.get_file_info(gdrive_info, "Name")                               │
│   219 │   │   │   mime_type = gdrive.get_file_info(gdrive_info, "Mime")                          │
│   220                                                                                            │
│                                                                                                  │
│ /home/alper/ebloc-broker/broker/drivers/gdrive.py:197 in pre_data_check                          │
│                                                                                                  │
│   194 │   def pre_data_check(self, key):                                                         │
│   195 │   │   if self.data_transfer_in_to_download > self.data_transfer_in_requested:            │
│   196 │   │   │   # TODO: full refund                                                            │
│ ❱ 197 │   │   │   raise Exception(                                                               │
│   198 │   │   │   │   "Requested size to download the source-code and data files is greater      │
│       than the given amount"                                                                     │
│   199 │   │   │   )                                                                              │
│   200                                                                                            │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
Exception: Requested size to download the source-code and data files is greater than the given amount
#+end_src

data_transfer_in=232 MB -> rounded=232 MB da sorun var 0 olunca calisiyor gdirve da ama

#+begin_src python
==> job_info={
    │   'provider': '0x4934a70Ba8c1C3aCFA72E809118BDd9048563A24',
    │   'job_owner': '0xe01eda38f7b5146463872f0c769ac14885dbf518',
    │   'job_key': '916f36db95f06fa1407a285998342cdc',
    │   'index': 0,
    │   'availableCore': 4,
    │   'cacheType': [0, 0, 0, 0],
    │   'stateCode': 0,
    │   'start_timestamp': 0,
    │   'submitJob_received_job_price': 6042500,
    │   'data_transfer_in_input': [10, 621, 0, 0],
    │   'data_transfer_out_input': 5,
    │   'data_transfer_in': 10,
    │   'data_transfer_out': 5,
    │   'commitment_block_duration': 600,
    │   'price_core_min': 100000,
    │   'price_data_transfer': 100,
    │   'price_storage': 100,
    │   'price_cache': 100,
    │   'received_bn': 21007373,
    │   'core': [1],
    │   'received': 6166700,
    │   'run_time': [60],
    │   'cloudStorageID': [3, 3, 2, 2],
    │   'result_ipfs_hash': '',
    │   'code_hashes': [
        │   │   b'916f36db95f06fa1407a285998342cdc',
        │   │   b'7d3e4f3aff0730fcf6e24ce0c3e42ede',
        │   │   b'9d5d892a63b5758090258300a59eb389',
        │   │   b'dd0fbccccf7a198681ab838c67b68fbf'
        │   ],
    │   'storage_duration': [],
    │   'submitJob_sum_storage_payment_cent': 0.0,
    │   'submitJob_block_hash': '0xd2379c71e379bced8204d1e4a9936f3f4bf1002c0cea8eed017f0b16609f9cee',
    │   'submitJob_tx_hash': '0x76c2cc26bd710799a2b2d815876358d66255cf398cc72f5df53994701be2e564',
    │   'data_prices_set_block_numbers': [0, 0, 0, 0],
    │   'submitJob_gas_used': 277276,
    │   'submitJob_LogDataStorageRequest': [
        │   │   {
            │   │   │   'provider': '0x4934a70Ba8c1C3aCFA72E809118BDd9048563A24',
            │   │   │   'owner': '0xE01eDA38f7b5146463872F0C769AC14885DBF518',
            │   │   │   'requestedHash': b'916f36db95f06fa1407a285998342cdc',
            │   │   │   'paid': 0
            │   │   }
        │   ],
    │   '_LogDataStorageRequest': [0],
    │   'processPayment_bn': 0,
    │   'processPayment_gas_used': 0,
    │   'received_block': [],
    │   'is_cached': {
        │   │   '916f36db95f06fa1407a285998342cdc': False,
        │   │   '7d3e4f3aff0730fcf6e24ce0c3e42ede': False,
        │   │   '9d5d892a63b5758090258300a59eb389': False,
        │   │   'dd0fbccccf7a198681ab838c67b68fbf': False
        │   },


~/ebloc-broker/broker/eblocbroker_scripts/process_payment.py \
          916f36db95f06fa1407a285998342cdc 0 0 2 "" '[3, 3, 2, 2]' 1687811273 232 0 '[1]' '[60]'
#+end_src
** TODO Submit N-nodes workflow as whole(without partitioning) to a provider and make it run.
